{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7440731,"sourceType":"datasetVersion","datasetId":4330748},{"sourceId":7638924,"sourceType":"datasetVersion","datasetId":4334382}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install torch==2.1.0  torchvision==0.16.0 torchtext==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121\n! pip install torch_geometric==2.4\n! pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n! pip install sentence-transformers\n! pip install torcheval\n! pip install matplotlib\n! pip install pandas\n! pip install tensorboard","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:22:49.842619Z","iopub.execute_input":"2024-01-22T15:22:49.843073Z","iopub.status.idle":"2024-01-22T15:27:04.089575Z","shell.execute_reply.started":"2024-01-22T15:22:49.843034Z","shell.execute_reply":"2024-01-22T15:27:04.088205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch_geometric.data import HeteroData\nimport pandas as pd\nimport numpy as np \nfrom sklearn.metrics import f1_score\nimport torch \n\n    \nfrom torch_geometric.data import HeteroData\n# load data\nimport torch \ntrain_graph = torch.load('/kaggle/input/twibot22-pyggraph/TwiBot22_Graph_with_degreecounts_train_with_y.pt')\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-01-22T15:27:04.092609Z","iopub.execute_input":"2024-01-22T15:27:04.093445Z","iopub.status.idle":"2024-01-22T15:33:01.606757Z","shell.execute_reply.started":"2024-01-22T15:27:04.093398Z","shell.execute_reply":"2024-01-22T15:33:01.605886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define GNN Model\nfrom torch_geometric.nn import HGTConv, Linear\n\nfrom torch.nn import functional as F\n\nclass HGT(torch.nn.Module):\n    def __init__(self, hidden_channels, out_channels, num_heads, num_layers, node_types, data_metadata):\n        super().__init__()\n\n        self.lin_dict = torch.nn.ModuleDict()\n        for node_type in node_types:\n            self.lin_dict[node_type] = Linear(-1, hidden_channels)\n\n        self.convs = torch.nn.ModuleList()\n        for _ in range(num_layers):\n            conv = HGTConv(hidden_channels, hidden_channels, data_metadata,\n                           num_heads, group='sum')\n            self.convs.append(conv)\n\n        self.lin = Linear(hidden_channels, out_channels)\n\n    def forward(self, x_dict, edge_index_dict):\n        x_dict = {\n            node_type: self.lin_dict[node_type](x).relu_()\n            for node_type, x in x_dict.items()\n        }\n\n        for conv in self.convs:\n            x_dict = conv(x_dict, edge_index_dict)\n\n        return self.lin(x_dict['user'])\n    \n    \n    \nmodel = HGT(hidden_channels=256, out_channels=1, num_heads=8, num_layers=2, node_types=train_graph.node_types, data_metadata=train_graph.metadata())\n","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:33:01.608018Z","iopub.execute_input":"2024-01-22T15:33:01.608306Z","iopub.status.idle":"2024-01-22T15:33:01.768373Z","shell.execute_reply.started":"2024-01-22T15:33:01.608282Z","shell.execute_reply":"2024-01-22T15:33:01.767596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get weights after training\nimport gc \ndef init(model, optimizer, state_dict_path, data):\n    with torch.no_grad():\n        model.eval()\n        for node_type in data.node_types:\n            print(node_type)\n\n            loader = HGTLoader(\n                    data.cuda(),\n                    # Sample 512 nodes per type and per iteration for 4 iterations\n                    num_samples=num_neighbors,\n                    batch_size=64, #96 or 32 nodes\n                    input_nodes=node_type,\n                    num_workers=0,\n                    pin_memory=True,\n                    prefetch_factor=None,\n                )\n            minibatch = next(iter(loader))\n\n            model(minibatch.x_dict, minibatch.edge_index_dict)\n            model_and_optimizer = torch.load(state_dict_path)\n            model.load_state_dict(model_and_optimizer['model_state_dict'])\n            del loader\n            gc.collect()\n            \n            \n            optimizer.load_state_dict(model_and_optimizer['optimizer_state_dict'])\n   \n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:33:12.187445Z","iopub.execute_input":"2024-01-22T15:33:12.189637Z","iopub.status.idle":"2024-01-22T15:33:12.197318Z","shell.execute_reply.started":"2024-01-22T15:33:12.189606Z","shell.execute_reply":"2024-01-22T15:33:12.196289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create minibatch loader\nfrom torch_geometric.loader import HGTLoader\nbatch_size = 32\nnum_node_types = len(train_graph.node_types)\none_hop_neighbors = (20 * batch_size)//num_node_types # per relationship type\ntwo_hop_neighbors = (20 * 8 * batch_size)//num_node_types # per relationship type\n#three_hop_neighbors = (20 * 8 * 3 * batch_size)//num_node_types # per relationship type\nnum_neighbors = [one_hop_neighbors, two_hop_neighbors]\n\nfor node_type in train_graph.node_types:\n    train_graph[node_type].x = train_graph[node_type].x.float()\n    \n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train model \nfrom tqdm.auto import tqdm\nfrom datetime import datetime\nfrom torch.utils.tensorboard import SummaryWriter\nfrom pathlib import Path\n\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\ncriterion = torch.nn.BCEWithLogitsLoss()  # more numerically stable than standard BCE because of log sum exp trick https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nrun_folder = Path(f'runs/{timestamp}')\nwriter = SummaryWriter(run_folder)\n\nprint(\"a\")\nmodel.train()\n\ngc.collect()\nprint('b')\nmodel.cuda()\nsamples_seen = 7175872\n\ninit(model,optimizer, f'/kaggle/input/twibot22-hgt-models/model_samplesseen{samples_seen}.pt', train_graph)\nprint('c')\ntrain_graph=train_graph.cuda()\nprint('d')\nloader = HGTLoader(\n        train_graph,\n        num_samples=num_neighbors,\n        batch_size=32,\n        input_nodes=('user', torch.arange(0, len(train_graph['user'].y))),\n        num_workers=4,\n        pin_memory=True,\n        prefetch_factor=2,\n        shuffle=True\n    )\nfor epoch in range(1):\n    for i,minibatch in enumerate(loader):\n        optimizer.zero_grad()\n        out = model(minibatch.x_dict, minibatch.edge_index_dict)\n        loss = criterion(out, minibatch['user'].y.unsqueeze(1).float())\n        loss.backward()\n        optimizer.step()\n        \n        if i % 100 == 0 or i == len(loader) - 1:\n            writer.add_scalar('Loss/train', loss.item(), samples_seen+(i+1)*32)\n        \n        if i % 1000 == 0 or i == len(loader) - 1:\n            torch.save({\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    }, run_folder/f'model_samplesseen{samples_seen+(i+1)*32}.pt')","metadata":{},"execution_count":null,"outputs":[]}]}